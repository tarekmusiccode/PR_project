{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BXo2horOJycT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR = (\"/content/drive/MyDrive/HandGesture/images\")\n",
        "LABELS = os.listdir(DATASET_DIR)\n",
        "IMAGE_SIZE = (224, 224)\n",
        "\n",
        "IMAGES = []\n",
        "for label in LABELS:\n",
        "    label_dir = os.path.join(DATASET_DIR, label)\n",
        "    for img in tqdm(os.listdir(label_dir), desc=f\"Processing images for '{label}' --->\"):\n",
        "        img_np = cv2.imread(os.path.join(label_dir, img))\n",
        "        img_np = cv2.resize(img_np, IMAGE_SIZE)\n",
        "        IMAGES.append({\n",
        "            \"img\": img_np,\n",
        "            \"label\": label,\n",
        "            \"label_num\": LABELS.index(label)\n",
        "        })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaepVO8oO7BL",
        "outputId": "eb7b0176-4225-48a4-c2c6-8ad76f987bf6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images for 'peace' --->: 100%|██████████| 526/526 [00:06<00:00, 86.68it/s] \n",
            "Processing images for 'rock_on' --->: 100%|██████████| 531/531 [00:08<00:00, 62.32it/s]\n",
            "Processing images for 'fingers_crossed' --->: 100%|██████████| 504/504 [00:06<00:00, 80.37it/s] \n",
            "Processing images for 'up' --->: 100%|██████████| 504/504 [00:05<00:00, 94.78it/s] \n",
            "Processing images for 'thumbs' --->: 100%|██████████| 537/537 [00:06<00:00, 80.84it/s] \n",
            "Processing images for 'rock' --->: 100%|██████████| 508/508 [00:05<00:00, 95.26it/s] \n",
            "Processing images for 'scissor' --->: 100%|██████████| 527/527 [00:05<00:00, 92.89it/s] \n",
            "Processing images for 'paper' --->: 100%|██████████| 539/539 [00:06<00:00, 88.17it/s] \n",
            "Processing images for 'okay' --->: 100%|██████████| 540/540 [00:05<00:00, 94.96it/s] \n",
            "Processing images for 'call_me' --->: 100%|██████████| 527/527 [00:05<00:00, 87.88it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(IMAGES)"
      ],
      "metadata": {
        "id": "2DoA5cAGO7Da"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_IMAGES, y_IMAGES = train_test_split(IMAGES, test_size=0.2, random_state=42)\n",
        "X_train = np.array([img[\"img\"] for img in X_IMAGES])\n",
        "y_train = np.array([img[\"label_num\"] for img in X_IMAGES])\n",
        "X_test = np.array([img[\"img\"] for img in y_IMAGES])\n",
        "y_test = np.array([img[\"label_num\"] for img in y_IMAGES])"
      ],
      "metadata": {
        "id": "Y-M67J7gO7Fz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = preprocess_input(X_train)\n",
        "X_test = preprocess_input(X_test)\n",
        "y_train_encoded = to_categorical(y_train, num_classes=len(LABELS))\n",
        "y_test_encoded = to_categorical(y_test, num_classes=len(LABELS))"
      ],
      "metadata": {
        "id": "OZS6K3nHZTlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(*IMAGE_SIZE, 3))\n",
        "base_model.trainable = False  # Freeze base layers\n",
        "\n",
        "# Add custom layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(len(LABELS), activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Bho1JmdQO7JO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train_encoded, epochs=10, validation_data=(X_test, y_test_encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbMyI6_SP2TH",
        "outputId": "51526421-3b22-4b95-8d1c-626461c00cc9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 2s/step - accuracy: 0.6391 - loss: 1.1177 - val_accuracy: 0.9800 - val_loss: 0.0897\n",
            "Epoch 2/10\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 1s/step - accuracy: 0.9518 - loss: 0.1840 - val_accuracy: 0.9933 - val_loss: 0.0430\n",
            "Epoch 3/10\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 2s/step - accuracy: 0.9682 - loss: 0.1153 - val_accuracy: 0.9943 - val_loss: 0.0238\n",
            "Epoch 4/10\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 1s/step - accuracy: 0.9788 - loss: 0.0820 - val_accuracy: 0.9971 - val_loss: 0.0175\n",
            "Epoch 5/10\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 1s/step - accuracy: 0.9788 - loss: 0.0714 - val_accuracy: 0.9952 - val_loss: 0.0197\n",
            "Epoch 6/10\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 1s/step - accuracy: 0.9732 - loss: 0.0848 - val_accuracy: 0.9943 - val_loss: 0.0158\n",
            "Epoch 7/10\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 1s/step - accuracy: 0.9902 - loss: 0.0428 - val_accuracy: 0.9971 - val_loss: 0.0144\n",
            "Epoch 8/10\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 1s/step - accuracy: 0.9862 - loss: 0.0405 - val_accuracy: 0.9962 - val_loss: 0.0117\n",
            "Epoch 9/10\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 1s/step - accuracy: 0.9919 - loss: 0.0276 - val_accuracy: 0.9962 - val_loss: 0.0127\n",
            "Epoch 10/10\n",
            "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9930 - loss: 0.0261 - val_accuracy: 0.9981 - val_loss: 0.0085\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f07093427a0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)\n",
        "pred = np.argmax(predictions, axis=1)\n",
        "accuracy = accuracy_score(y_test, pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T85fppstRQyI",
        "outputId": "90fd6f9f-0b40-4ab6-a9fb-b2eca47ab17d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step\n",
            "Accuracy: 0.998093422306959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test, pred, target_names=LABELS)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXf2i4a7RRjh",
        "outputId": "2abfe163-fe39-47f6-de62-8840fb64d290"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "          peace       0.99      1.00      1.00       114\n",
            "        rock_on       1.00      1.00      1.00       117\n",
            "fingers_crossed       1.00      0.99      1.00       104\n",
            "             up       1.00      1.00      1.00        88\n",
            "         thumbs       1.00      1.00      1.00       108\n",
            "           rock       1.00      1.00      1.00       100\n",
            "        scissor       1.00      1.00      1.00       113\n",
            "          paper       0.99      1.00      1.00       107\n",
            "           okay       1.00      0.99      0.99        94\n",
            "        call_me       1.00      1.00      1.00       104\n",
            "\n",
            "       accuracy                           1.00      1049\n",
            "      macro avg       1.00      1.00      1.00      1049\n",
            "   weighted avg       1.00      1.00      1.00      1049\n",
            "\n"
          ]
        }
      ]
    }
  ]
}